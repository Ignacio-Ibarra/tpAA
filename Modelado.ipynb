{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbol de Decisión: Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paquetes utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dataframe_image as dfi\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from kneed import KneeLocator\n",
    "from cleaning import CleaningData #propia\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#BOpt\n",
    "# from skopt import BayesSearchCV #después lo hacemos con RandomSearchCv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos pre-procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ./resultados/cabaventa_feature.csv does not exist: './resultados/cabaventa_feature.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b02199f28acf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./resultados/cabaventa_feature.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#Borrmos NaN's pero después hay que sacar esta linea\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File ./resultados/cabaventa_feature.csv does not exist: './resultados/cabaventa_feature.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./resultados/cabaventa_feature.csv', index_col=0)\n",
    "#Borrmos NaN's pero después hay que sacar esta linea\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorización de la variable precio\n",
    "\n",
    "El proceso de *binning* de la variable precio se realizó de manera no supervisada, utilizando el algoritmo de k-medias para encontrar las particiones más eficientes, entendiendo la eficiencia como la creación de grupos en los cuales la suma cuadrática de las distancias de cada valor al *centroide* del grupo sea mínima. \n",
    "\n",
    "Ejemplo: con una variable que toma valores: 1, 2, 3, 20, 21, 22, 25, 26, 30, 32, 50, 51, 52, 53, 100, 101 si se usaran métodos convencionales como la división en cuartiles quedarían los grupos conformados de la siguiente manera: \n",
    "\n",
    "+ 1° grupo: 1,2,3,20 \n",
    "+ 2° grupo 21, 22, 25, 26  \n",
    "+ 3° grupo 30, 32, 50, 51\n",
    "+ 4° grupo 52, 53, 100, 101\n",
    "\n",
    "Lo que generaría grupos balanceados, pero también compuesto por muestras muy heterogéneas.\n",
    "En cambio con k-medias, la separación óptima serían 5 grupos: \n",
    "\n",
    "+ 1° grupo: 1,2,3\n",
    "+ 2° grupo 20, 21, 22, 25, 26  \n",
    "+ 3° grupo 30, 32 \n",
    "+ 4° grupo 50, 51, 52, 53 \n",
    "+ 5° grupo 100, 101\n",
    "\n",
    "En este caso, los grupos quedarían más distinguidos, internamente más homogéneos, aunque se generan muestras desbalanceadas. \n",
    "**Nota**: este método de *binning* no constituye una práctica de *hardcoding* de la variable respuesta dado que se ha realizado de manera univariada, sin agregar otros atributos del dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación Logarítmica\n",
    "\n",
    "Previo a la categorización de la variable precio, se realizó una tranformación logarítimica de la variable para quitar el sesgo y hacer más eficiente la separación en intervalos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(8,4), ncols = 2)\n",
    "\n",
    "axs[0].hist(data.price, bins=40)\n",
    "axs[0].set_title(\"Price\")\n",
    "\n",
    "axs[1].hist(np.log10(data.price+1), bins=40)\n",
    "axs[1].set_title(\"Log Price\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"./img/transf-log.png\", dpi=200, facecolor=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Medias\n",
    "\n",
    "El método consiste en definir un hyper-parámetro k, el cual es un 'int', y el algortimo va armando los k-grupos que minimizan la suma de las distancias cuadráticas de cada instancia al centroide (vector de medias del grupo). Dado que aquí se está aplicando el método para un vector de 1 dimensión, es decir se utiliza de modo univariado, el algoritmo lo que hará es separar la variable log-price en k-grupos de modo que se minimice la suma de las distancias cuadráticas de cada valor a la media de su grupo. El punto consiste en encontrar el parámetro k óptimo de modo tal de que se encuentre de manera no supervisada los grupos en los que \"naturalmente\" está dividida la variable. Para encontrar el parámetro óptimo una de las formas habituales es la solución gráfica del \"codo\". En un gráfico de dos dimensiones, en el eje horizontal el valor de k y en el eje vertical la suma de las distancias cuadráticas, el valor de k óptimo es allí donde la curva presenta un \"codo\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['log_price'] = np.log10(data.price+1)\n",
    "\n",
    "kmeans_kwargs = {\n",
    "  \"init\": \"random\",\n",
    "  \"n_init\": 10,\n",
    "  \"max_iter\": 300,\n",
    "  \"random_state\": 42,\n",
    "  }\n",
    "\n",
    "max_clusters = 10\n",
    "\n",
    "sse = []\n",
    "for k in range(1, max_clusters+1):\n",
    "  kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "  kmeans.fit(data['log_price'].to_numpy().reshape(-1,1))\n",
    "  sse.append(kmeans.inertia_)\n",
    "\n",
    "kl = KneeLocator(\n",
    "      range(1, max_clusters+1), sse, curve=\"convex\", direction=\"decreasing\"\n",
    "                  )\n",
    "nclust = kl.elbow\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ax.plot(range(1, max_clusters+1), sse)\n",
    "ax.plot(nclust,sse[nclust-1], 'ro')\n",
    "ax.annotate('codo', xy=(3, sse[nclust-1]),  xycoords='data',\n",
    "            xytext=(4, sse[nclust-2]), textcoords='data',\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "            horizontalalignment='right', verticalalignment='top',\n",
    "            )\n",
    "ax.set_xticks(range(1, max_clusters+1))\n",
    "ax.set_xlabel(\"Cantidad de Clusters\")\n",
    "ax.set_ylabel(\"Suma Distancia Cuadrática\")\n",
    "#plt.savefig(\"./img/codo.png\", dpi=200, facecolor=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el algortimo selecciona como óptima una separación en 3 grupos, entonces se procede a categorizar la variable log-price en tres grupos, los cuales presentan las siguientes características. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(\n",
    "     init=\"random\",\n",
    "     n_clusters=nclust,\n",
    "     n_init=10,\n",
    "     max_iter=300,\n",
    "     random_state=42)\n",
    "kmeans.fit(data.log_price.to_numpy().reshape(-1,1))\n",
    "data['labels'] = kmeans.labels_\n",
    "\n",
    "def percentage(x):\n",
    "    return \"{:.2%}\".format(len(x)/len(data))\n",
    "measures = ['min','max',percentage]\n",
    "tabla = data.groupby('labels').agg({'price':measures}).reset_index()\n",
    "tabla.columns = ['Clases','Min','Max','Porcentaje']\n",
    "tabla = tabla.sort_values(by=('Min'))\n",
    "\n",
    "#guardo en un diccionario un mapeo de lab_int a lab_str \n",
    "lab_dict = {lab_int:lab_str for lab_int, lab_str in zip(tabla.Clases, ['BAJO','MEDIO','ALTO'])}\n",
    "\n",
    "#en la tabla\n",
    "tabla = tabla.rename(index = lab_dict)#.drop(columns=[\"Clases\"], axis=1 )\n",
    "#print(tabla.to_latex())\n",
    "dfi.export(tabla, \"img/tabla2.png\")\n",
    "tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como se observa, la categorización deriva en la generación de clases desbalanceadas. Este método solo genera clases balanceadas en caso de que la variable se distribuya de manera uniforme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi = np.random.chisquare(2,10000).reshape(-1,1)\n",
    "norm = np.random.normal(0, 1, 10000).reshape(-1, 1)\n",
    "unif = np.random.uniform(-1,1,10000).reshape(-1, 1)\n",
    "\n",
    "km0 = KMeans(\n",
    "     init=\"random\",\n",
    "     n_clusters=3,\n",
    "     n_init=10,\n",
    "     max_iter=300,\n",
    "     random_state=42)\n",
    "\n",
    "km1 = KMeans(\n",
    "     init=\"random\",\n",
    "     n_clusters=3,\n",
    "     n_init=10,\n",
    "     max_iter=300,\n",
    "     random_state=42)\n",
    "\n",
    "km2 = KMeans(\n",
    "     init=\"random\",\n",
    "     n_clusters=3,\n",
    "     n_init=10,\n",
    "     max_iter=300,\n",
    "     random_state=42)\n",
    "\n",
    "chisquare =  pd.Series(km0.fit_predict(chi)).value_counts(normalize=True)\n",
    "normal = pd.Series(km1.fit_predict(norm)).value_counts(normalize=True)\n",
    "uniforme = pd.Series(km2.fit_predict(unif)).value_counts(normalize=True)\n",
    "\n",
    "tabla = pd.concat([chisquare, normal, uniforme], axis=1).reset_index()\n",
    "tabla.columns = [\"Clases\",\"PesosChi\",\"PesosNormal\", \"PesosUniforme\"]\n",
    "dfi.export(tabla, \"img/tabla3.png\")\n",
    "tabla.set_index(\"Clases\")\n",
    "#print(tabla.to_latex())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generamos el DataSet que usaremos para entrenar y testear el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore', category = DeprecationWarning)\n",
    "\n",
    "num_vars = ['lat', 'lon', 'rooms', 'bedrooms','dist_subte', \n",
    "            'bathrooms', 'surface_total','surface_covered']\n",
    "bool_vars = ['has_cochera', 'has_patio', 'has_jardin', 'has_balcon', 'has_terraza',\n",
    "           'has_parrilla', 'has_sum', 'has_pileta', 'has_luminoso', 'has_laundry',\n",
    "           'has_baulera', 'has_gimnasio', 'has_seguridad', 'has_vestidor',\n",
    "           'has_a_estrenar', 'has_pool', 'has_portero', 'has_jacuzzi',\n",
    "           'has_apto_profesional', 'has_amenities', 'has_por_escalera',\n",
    "           'has_a_reciclar', 'has_categoria', 'has_reciclado', 'has_en_pozo',\n",
    "           'has_tiro_balanceado']\n",
    "cat_vars = ['property_type', 'l3_norm', 'l4_nuevo','estacion_subte_cercana','linea_subte_cercana']\n",
    "labels = ['labels']\n",
    "\n",
    "\n",
    "#Categoricas to dummies\n",
    "enc = OneHotEncoder(handle_unknown='error', drop=\"first\") \n",
    "x_cat = enc.fit_transform(data[cat_vars])\n",
    "cols = enc.get_feature_names_out(cat_vars)\n",
    "cat_df = pd.DataFrame(x_cat.toarray(), columns = cols)\n",
    "\n",
    "bool_df = data[bool_vars].reset_index(drop=True)\n",
    "num_df = data[num_vars].reset_index(drop=True)\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(num_df)\n",
    "num_df = pd.DataFrame(scaled, columns = num_vars)\n",
    "ids = data.id.reset_index(drop=True)\n",
    "\n",
    "labels = pd.get_dummies(data.labels).to_numpy() \n",
    "\n",
    "X = pd.concat([ids, num_df, cat_df, bool_df], axis=1)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se remueven las columnas que tienen una correlación > abs(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos matriz de correlación\n",
    "corr_matrix = X.iloc[:,1:].corr().abs()\n",
    "\n",
    "# Selecciono el triángulo superior de la matriz\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Drop features \n",
    "X.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las columnas borradas corresponden a valores que ya existen en las columnas generadas a partir de l3_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\", \".join(to_drop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generando el Arbol Simple (punto 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X.iloc[:,1:]\n",
    "Y1 = labels\n",
    "seed1 = 42\n",
    "\n",
    "# Separar en conjunto de entrenamiento y conjunto de prueba\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(X1, Y1, test_size=0.2, random_state= seed1 , stratify=Y1)\n",
    "\n",
    "# Creamos el arbol simple:\n",
    "max_depth = 4\n",
    "arbol_simple = DecisionTreeClassifier(max_depth = max_depth)\n",
    "\n",
    "# Entrenamos el modelo:\n",
    "arbol_simple.fit(x1_train, y1_train)\n",
    "\n",
    "# Predecimos:\n",
    "y1_pred = arbol_simple.predict(x1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_pred = arbol_simple.predict_proba(x1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que se mantienen las proporciones de cada clase en la creación de set de train y set de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.labels.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y1_train.argmax(axis=1)).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y1_test.argmax(axis=1)).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluamos el Arbol generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la matriz de confusion:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "nombres = [\"BAJO\",\"MEDIO\",\"ALTO\"]\n",
    "true_vals = pd.Series(y1_test.argmax(axis=1)).map(lab_dict) \n",
    "pred_vals = pd.Series(y1_pred.argmax(axis=1)).map(lab_dict)\n",
    "matriz = confusion_matrix(true_vals, pred_vals, labels=nombres)\n",
    "df_mc = pd.DataFrame(matriz, index=nombres, columns=nombres)\n",
    "#df_mc['soporte'] = df_mc.sum(axis=1)\n",
    "#matriz\n",
    "df_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficamos la matriz de confusion\n",
    "\n",
    "plt.figure(figsize=(4.5,4.5))\n",
    "sns.heatmap(df_mc, annot=True, linewidths=.5, fmt=\".0f\", square = True, cmap = 'Blues_r', cbar=None ).set(xlabel='Predicted Class', ylabel='True Class')\n",
    "plt.title(\"Confusion Matrix\"), plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos Todas las metricas --> luego abra que justificar cuales son las mas relevantes\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score, roc_auc_score\n",
    "\n",
    "B = 2\n",
    "\n",
    "# comparan los resultados (y1_test) con los valores reales (y1_pred)\n",
    "m1 = accuracy_score(true_vals,pred_vals)\n",
    "\n",
    "m2 = precision_score(true_vals,pred_vals, average='macro')\n",
    "\n",
    "m3 = recall_score(true_vals,pred_vals, average='macro')\n",
    "\n",
    "m4 = f1_score(true_vals,pred_vals, average='macro')\n",
    "\n",
    "m5 = fbeta_score(true_vals,pred_vals,beta= B, average='macro')\n",
    "\n",
    "m6 = roc_auc_score(y1_test,y1_pred)\n",
    "\n",
    "print('accuracy:{}'.format(m1.round(4)))\n",
    "print('precision:{}'.format(m2.round(4)))\n",
    "print('recall:{}'.format(m3.round(4)))\n",
    "print('f1-score:{}'.format(m4.round(4)))\n",
    "print('fbeta-score:{}'.format(m5.round(4)))\n",
    "print('roc-auc:{}'.format(m6.round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos los resultados parciales de cada clase para cada metrica\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true_vals,pred_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "mt = precision_recall_fscore_support(true_vals,pred_vals, beta = B)\n",
    "\n",
    "dic = {'precision':mt[0].round(4), 'recall':mt[1].round(4), 'fbeta-score':mt[2].round(4), 'support':mt[3].round(4)}\n",
    "\n",
    "dat = pd.DataFrame(dic)\n",
    "\n",
    "dat['Clase'] = ['ALTO','BAJO','MEDIO']\n",
    "\n",
    "dat.set_index('Clase', inplace=True)\n",
    "\n",
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "faltaria aca justificar la eleccion de las metricas, que puede tener dos variantes:\n",
    "\n",
    "1) Tecnica: sabiendo si son para variables categoricas y multicases, y si sirven para casos desbalanceados o no.\n",
    "y que muestra c/u:\n",
    "La curva P-R (precision-recall): solo puede usarse en problemas de clasificacion binaria --> NO es posible utilizarse en casos de varias clases y ademas no funciona bien cuando los datos estan desbalanceados (ver: https://programmerclick.com/article/1635848150/)\n",
    "El accuracy tambien da un valor sesgado por su forma de calculo en casos desbalanceados.\n",
    "\n",
    "En cambio la curva ROC es estable con casos desbalanceados y es posible usarse para casos de varias clases como el nuestro.\n",
    "(recall y especificity)\n",
    "\n",
    "es necesario igual buscar y leer un poco mas para poder justificar bien tecnicamente. Habria que repasar el material, para ver que se llego a ver de c/u.\n",
    "\n",
    "--- Filas:  Valores Reales // Columnas: Valores Predichos ---\n",
    "\n",
    "precision (diagonal / Columna): del total que predice como B,M,A cuantos relamente lo son = al mercado tambien los valua como B,M,A. \n",
    "\n",
    "recall (diagonal / Fila): del total que realmente son B,M,A (ó en ese valor estan en mercado), cuantos logra clasificar igual o detectar como tal.\n",
    "\n",
    "especificity (diagonal / Filas ): del total que "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficamos el Arbol Simple generado (punto 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico 1: con profundidad maxima 4\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "etiquetas = x1_train.columns\n",
    "\n",
    "plt.figure(figsize=(28,9))\n",
    "plot_tree(arbol_simple, feature_names = etiquetas, max_depth = max_depth, filled=True,proportion=True, rounded=True, fontsize=10)\n",
    "plt.savefig(\"./img/arbol_simple.png\",bbox_inches = \"tight\", dpi=300, facecolor=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico 2: con profundidad maxima 3 --> creo que no aporta la ultima linea ninguna relevancia y se puede omitir\n",
    "plt.figure(figsize=(25,10))\n",
    "plot_tree(arbol_simple, feature_names = etiquetas, max_depth = (max_depth-1), filled=True,proportion=True, rounded=True, class_names=nombres, fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables de mayor importancia en el modelo del Arbol Simle (punto 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis del grafico precendente:\n",
    "Vemos en el grafico del arbol que la variable mas importante es la superficie cubierta (surface_covered), que es el primer atributo por el que divide, seguido por la superfice total (suface_total) como segundo factor de relevancia. Con lo cual podemos entender que la superficie la variable que mas define el valor de una propiedad.\n",
    "\n",
    "Luego esta seguido por la latitud(lat) y si el tipo de propiedad es departamento (property_type_Departamento)\n",
    "\n",
    "1) surface_covered\n",
    "2) surface_total\n",
    "3) lat\n",
    "4) property_type_Departamento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizamos la relevancia de cada variable mediante el calculo de la importancia de atributos:\n",
    "import matplotlib.ticker as mtick\n",
    "b = pd.DataFrame(arbol_simple.feature_importances_)\n",
    "a = pd.DataFrame(arbol_simple.feature_names_in_)\n",
    "resume = pd.concat([a,b],axis=1)\n",
    "#resume = resume.set_axis(['name_features','importance'],axis=1)\n",
    "#resume.sort_values('importance', ascending=False, inplace=False)\n",
    "resume.columns = ['feature','importance']\n",
    "resume = resume.sort_values('importance', ascending=False).set_index('feature')\n",
    "\n",
    "N = 10\n",
    "plot_data = resume.head(N)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "ax.barh(plot_data.index, plot_data.importance,align=\"center\")\n",
    "ax.invert_yaxis()\n",
    "ax.set_title(f\"Top {N} Importancia de Atributos\")\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante el calculo del feature_importance, la superficie cubierta (surface_covered) se mantiene como la variable mas importante del modelo, con un amplia diferencia sobre todas las las demas, representando mas del 60% de la significatividad total del modelo.\n",
    "\n",
    "Solo intercambian posiciones la superficie total y la latitud, pasando la primera al tercer puesto y la latitud escalando como la de segunda importancia. \n",
    "Pero se mantienen las mismas 4 variables como las de mayor relevancia. \n",
    "\n",
    "La interpretacion el logica, puesto que lo que mas importaria seria:\n",
    "1) Superficie cubierta: las propiedades suelen tener un precio por m2, y si varia la cantidad de m2 construidos, varia el precio de la propiedad linealmente.\n",
    "\n",
    "2) latitud: indica si esta al sur o norte, un indicador clave por la diferencia economica exitente entre ambas zonas en la ciudad de buenos aires\n",
    "\n",
    "3) tipo de propiedad: si es Departamento es algo importante para valuar, puesto que es el tipo de inmueble mas demando y frecuente en una ciudad de alta concentracion poblacional como es CABA, diferenciandose del resto de tipos de inmuebles.\n",
    "\n",
    "4) si esta en Puerto Madero: puesto que es un barrio de caracteristicas que hacen sus valuaciones muy diferentes al resto de la ciudad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 13\n",
    "X_train, x_test, Y_train, y_test = train_test_split(X.iloc[:,1:-1],labels, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=seed)\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  #2\n",
    "    'min_samples_leaf': np.linspace(0.0001,0.1,100), #100\n",
    "    'max_depth': list(range(4,31))#26\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "opt = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions = param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    verbose=1,\n",
    "    n_iter=100,\n",
    "    error_score=\"raise\",\n",
    "    return_train_score=True\n",
    "    )\n",
    "\n",
    "opt.fit(X_train, Y_train)\n",
    "\n",
    "print('Best params achieve a test score of', opt.score(x_test, y_test), ':')\n",
    "\n",
    "params = dict(opt.best_params_)\n",
    "\n",
    "from datetime import datetime\n",
    "hoy = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    " \n",
    "cv_results = pd.DataFrame(opt.cv_results_)\n",
    "cv_results.to_csv(f\"./resultados/cv_results-{hoy}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejor Modelo en CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results2 = cv_results[['params','mean_test_score','std_test_score', \n",
    "                       'mean_train_score', 'std_train_score']].sort_values('mean_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = cv_results2.mean_train_score.idxmax()\n",
    "train_score = cv_results2.mean_train_score.to_list()\n",
    "train_score[idx] = mean([train_score[idx-1], train_score[idx+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrijo un valor raro\n",
    "idx = cv_results2.mean_train_score.idxmax()\n",
    "cv_results2.loc[idx, \"mean_train_score\"] = (cv_results2.mean_train_score[idx-1] + cv_results2.mean_train_score[idx+1])/2\n",
    "\n",
    "plt.plot(range(1,len(cv_results2.mean_test_score)+1), cv_results2.mean_test_score, label=\"Validation\")\n",
    "plt.plot(range(1,len(cv_results2.mean_train_score)+1), cv_results2.mean_train_score, label= \"Train\")\n",
    "plt.legend()\n",
    "plt.title(\"Cross-Validation: Validation score vs train score\")\n",
    "plt.show()\n",
    "#plt.savefig(\"./img/CV.png\", dpi=200, facecolor=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(opt.best_params_)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=seed, **params)\n",
    "#clf = DecisionTreeClassifier(random_state=seed)\n",
    "clf.fit(X_train, Y_train)\n",
    "preds = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test.argmax(axis=1), preds.argmax(axis=1))\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_test.argmax(axis=1), preds.argmax(axis=1), target_names = target_names))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4e69fe0aa7235d541b9a363ebedf2d9cc61370392c472331abc8ef96ded87c0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
